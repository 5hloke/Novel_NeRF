{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import imageio\n",
    "# Make sure to switch runtime to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a class to read in the image frame and store all the data related to it \n",
    "class FrameManager:\n",
    "    def __init__(self):\n",
    "        self.test_frames = []\n",
    "        self.train_frames = []\n",
    "        self.val_frames = []\n",
    "        self.cam_angle = 0\n",
    "        self.f = None\n",
    "        self.H = None\n",
    "        self.W = None\n",
    "    \n",
    "    def read_frames(self, path):\n",
    "        cfgs = ['transforms_test.json', 'transforms_train.json', 'transforms_val.json']\n",
    "        data_cfg = {}\n",
    "        for i, cfg in enumerate(cfgs):\n",
    "            with open(os.path.join(path, cfg)) as json_file:\n",
    "                data_cfg[i] = json.load(json_file)\n",
    "        \n",
    "        for i in range(3):\n",
    "            data = data_cfg[i]\n",
    "            frms = []\n",
    "            for frame in data[\"frames\"]:\n",
    "                img_file = os.path.join(basedir, frame['file_path'] + '.png')\n",
    "                self.cam_angle = data_cfg[0]['camera_angle_x']\n",
    "                img = imageio.imread(img_file)\n",
    "                self.H, self.W = img.shape[0], img.shape[1]                  \n",
    "                '''It's basic geometry: you have a right angle triangle, with half the FOV as one of the angles (a), and half your image size as the opposite side (A). To calculate the focal length F, use tan(a) = A/F,\n",
    "which gives F = A/tan(a)'''\n",
    "                self.f = (0.5 * self.W)/(np.tan(0.5 * self.cam_angle))\n",
    "                new_frame = Frame(img, np.array(frame['transform_matrix'], self.f))\n",
    "                frms.append(new_frame)\n",
    "            if i == 0:\n",
    "                self.test_frames = frms\n",
    "            elif i == 1:\n",
    "                self.train_frames = frms\n",
    "            else:\n",
    "                self.val_frames = frms\n",
    "\n",
    "\n",
    "class Frame:\n",
    "    def __init__(self, image, pose, f):\n",
    "        self.img = image\n",
    "        self.pose = pose\n",
    "        self.H, self.W = image.shape[0], image.shape[1]\n",
    "        self.f = f\n",
    "        self.samples = None\n",
    "        self.rays_o = None\n",
    "        self.rays_d = None\n",
    "        self.depth_values = None\n",
    "\n",
    "    def copy_to_device(self, device):\n",
    "        self.img = torch.from_numpy(self.img).to(device)\n",
    "        self.pose = torch.from_numpy(self.pose).to(device)\n",
    "    # function to get the rays from the image through every pixel of the Camera (Using Pytorch) on GPU\n",
    "    # Assuming a pinhole camera model\n",
    "    def get_rays(self, device):\n",
    "        self.copy_to_device(device)\n",
    "\n",
    "        i, j = torch.meshgrid(torch.arange(self.H).to(device), torch.arange(self.W).to(device), indexing='ij')\n",
    "        i, j = i.transpose(1, 0), j.transpose(1, 0)\n",
    "        dirs = torch.stack([(i-self.W*0.5)/self.f, -(j-self.H*0.5)/self.f, -torch.ones_like(i)], -1)\n",
    "\n",
    "        rays_d = torch.sum(dirs[..., None, :] * self.pose[:3, :3], -1)\n",
    "        rays_o = torch.broadcast_to(self.pose[:3, -1], rays_d.shape)\n",
    "        self.rays_o = rays_o\n",
    "        self.rays_d = rays_d\n",
    "        \n",
    "        return rays_o, rays_d\n",
    "        \n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a list of frames and samples all the rays for the frames uniformly and returns the rays and the depth values\n",
    "# could make it a method of the FrameManager class\n",
    "def sample_frame(frames, num_samples, near, far, dev = 'cuda:0'):\n",
    "    \n",
    "    sample_space = torch.linspace(0., 1., num_samples, device=dev)\n",
    "    depth = near*(1.-sample_space) + far*sample_space\n",
    "    mid_depth = (depth[1:] + depth[:-1])/2\n",
    "    rand_sampling = torch.rand([num_samples], device=dev)\n",
    "    upper_sample = torch.cat([mid_depth, depth[-1:]], dim=-1)\n",
    "    lower_sample = torch.cat([depth[:1], mid_depth], dim=-1)\n",
    "    depth_value = lower_sample + rand_sampling * (upper_sample - lower_sample)\n",
    "    depth_value = depth_value.expand(list(frames.rays_o.shape[:-1]) +[num_samples])\n",
    "    #pts are the points on the ray in the format (width, height, n_samples, 3)\n",
    "    pts = frames.rays_o[..., None, :] + frames.rays_d[..., None, :] * depth_value[..., None]\n",
    "    frames.samples = pts\n",
    "    frames.depth_values = depth_value\n",
    "    return pts, depth_value\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60])\n",
      "torch.Size([24])\n",
      "torch.Size([280])\n",
      "(tensor([0.4613, 0.5245, 0.4882], grad_fn=<SigmoidBackward0>), tensor(0., grad_fn=<SelectBackward0>))\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(60, 256)\n",
    "        self.hidden_layer_block_1 = nn.ModuleList([nn.Linear(256, 256) for i in range(4)])\n",
    "        self.skip_connection_layer = nn.Linear(316,256)\n",
    "        self.hidden_layer_block_2 = nn.ModuleList([nn.Linear(256, 256) for i in range(2)])\n",
    "        self.density_output_layer = nn.Linear(256, 257)\n",
    "        self.last_layer = nn.Linear(280, 128)\n",
    "        self.rgb_output = nn.Linear(128, 3)\n",
    "\n",
    "\n",
    "    def forward(self, position, direction):\n",
    "        \n",
    "        encoded_position, encoded_direction = self.positional_encoding(position, direction)\n",
    "        \n",
    "        print(encoded_position.size())\n",
    "        print(encoded_direction.size())\n",
    "\n",
    "        input_feature_origin = encoded_position.clone()\n",
    "\n",
    "        x = torch.relu(self.input_layer(encoded_position))\n",
    "        for layer in self.hidden_layer_block_1:\n",
    "            x = torch.relu(layer(x))\n",
    "\n",
    "        skip_connection = torch.cat((input_feature_origin, x), dim=0)\n",
    "        x = torch.relu(self.skip_connection_layer(skip_connection))\n",
    "\n",
    "        for layer in self.hidden_layer_block_2:\n",
    "            x = torch.relu(layer(x))\n",
    "\n",
    "        x = torch.relu(self.density_output_layer(x))\n",
    "\n",
    "        density = x[-1]\n",
    "        direction_connection = torch.cat((encoded_direction, x[:-1]), dim=0)\n",
    "        print(direction_connection.size())\n",
    "        x = torch.relu(self.last_layer(direction_connection))\n",
    "        color = torch.sigmoid(self.rgb_output(x))\n",
    "\n",
    "        return color, density\n",
    "\n",
    "    def positional_encoding(self, position, direction, L_P = 10, L_D =4):\n",
    "        \n",
    "        encoded_position = []\n",
    "        encoded_direction = []\n",
    "\n",
    "        for pos in position:\n",
    "            for i in range (L_P):\n",
    "                encoded_position.append(torch.sin(2**i * np.pi * pos))\n",
    "                encoded_position.append(torch.cos(2**i * np.pi * pos))\n",
    "        \n",
    "        for direct in direction:\n",
    "            for i in range (L_D):\n",
    "                encoded_direction.append(torch.sin(2**i * np.pi * direct))\n",
    "                encoded_direction.append(torch.cos(2**i * np.pi * direct))\n",
    "\n",
    "        return [torch.FloatTensor(encoded_position), torch.FloatTensor(encoded_direction)]\n",
    "\n",
    "\n",
    "model = Model()\n",
    "output = model(torch.rand(3), torch.rand(3))\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
