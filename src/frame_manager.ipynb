{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xWmicwg-WJXO","executionInfo":{"status":"ok","timestamp":1701034167540,"user_tz":300,"elapsed":805,"user":{"displayName":"Shlok Agarwal","userId":"03636398256053486916"}},"outputId":"dc8a7166-38e8-4d18-a684-dbe0100f9ed5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Running on the GPU\n","cuda:0\n"]}],"source":["import torch\n","import os\n","from torchvision import transforms, datasets\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import json\n","import imageio\n","from google.colab import drive\n","drive.mount('/content/drive')\n","# Make sure to switch runtime to the GPU\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\")\n","    print(\"Running on the GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Running on the CPU\")\n","print(device)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"74S6zImrWJXQ","executionInfo":{"status":"ok","timestamp":1701034168499,"user_tz":300,"elapsed":151,"user":{"displayName":"Shlok Agarwal","userId":"03636398256053486916"}}},"outputs":[],"source":["# Write a class to read in the image frame and store all the data related to it\n","class FrameManager:\n","    def __init__(self):\n","        self.test_frames = []\n","        self.train_frames = []\n","        self.val_frames = []\n","        self.cam_angle = 0\n","        self.f = None\n","        self.H = None\n","        self.W = None\n","\n","    def prepare_test_data(self):\n","      rays_o, rays_d = None, None\n","      for frame in self.test_frames:\n","        f_rays_o, f_rays_d = frame.get_rays(device)\n","        if rays_o is None:\n","          rays_o, rays_d = f_rays_o, f_rays_d\n","        else:\n","          rays_o = torch.vstack((rays_o, f_rays_o))\n","          rays_d = torch.vstack((rays_d, f_rays_d))\n","      return rays_o, rays_d\n","\n","    def prepare_train_data(self):\n","      rays_o, rays_d = None, None\n","      for frame in self.train_frames:\n","        f_rays_o, f_rays_d = frame.get_rays(device)\n","        if rays_o is None:\n","          rays_o, rays_d = f_rays_o, f_rays_d\n","        else:\n","          rays_o = torch.vstack((rays_o, f_rays_o))\n","          rays_d = torch.vstack((rays_d, f_rays_d))\n","      return rays_o, rays_d\n","\n","    def prepare_val_data(self):\n","      rays_o, rays_d = None, None\n","      for frame in self.val_frames:\n","        f_rays_o, f_rays_d = frame.get_rays(device)\n","        if rays_o is None:\n","          rays_o, rays_d = f_rays_o, f_rays_d\n","        else:\n","          rays_o = torch.vstack((rays_o, f_rays_o))\n","          rays_d = torch.vstack((rays_d, f_rays_d))\n","      return rays_o, rays_d\n","\n","\n","    def read_frames(self, path):\n","        cfgs = ['transforms_test.json', 'transforms_train.json', 'transforms_val.json']\n","        data_cfg = {}\n","        for i, cfg in enumerate(cfgs):\n","            with open(os.path.join(path, cfg)) as json_file:\n","                data_cfg[i] = json.load(json_file)\n","\n","        for i in range(3):\n","            data = data_cfg[i]\n","            frms = []\n","            for frame in data[\"frames\"]:\n","                img_file = os.path.join(path, frame['file_path'] + '.png')\n","                self.cam_angle = data_cfg[0]['camera_angle_x']\n","                img = imageio.imread(img_file)\n","                self.H, self.W = img.shape[0], img.shape[1]\n","                '''It's basic geometry: you have a right angle triangle, with half the FOV as one of the angles (a), and half your image size as the opposite side (A). To calculate the focal length F, use tan(a) = A/F,\n","which gives F = A/tan(a)'''\n","                self.f = (0.5 * self.W)/(np.tan(0.5 * self.cam_angle))\n","                new_frame = Frame(img, np.array(frame['transform_matrix']), self.f)\n","                frms.append(new_frame)\n","            if i == 0:\n","                self.test_frames = frms\n","            elif i == 1:\n","                self.train_frames = frms\n","            else:\n","                self.val_frames = frms\n","\n","\n","class Frame:\n","    def __init__(self, image, pose, f):\n","        self.img = image\n","        self.pose = pose\n","        self.H, self.W = image.shape[0], image.shape[1]\n","        self.f = f\n","        self.samples = None\n","        self.rays_o = None\n","        self.rays_d = None\n","        self.depth_values = None\n","\n","    def copy_to_device(self, device):\n","        self.img = torch.from_numpy(self.img).to(device)\n","        self.pose = torch.from_numpy(self.pose).to(device)\n","\n","    def make_tensors(self):\n","      self.img = torch.from_numpy(self.img)\n","      self.pose = torch.from_numpy(self.pose)\n","\n","    # function to get the rays from the image through every pixel of the Camera (Using Pytorch) on GPU\n","    # Assuming a pinhole camera model\n","    def get_rays(self, device):\n","        # self.copy_to_device(device)\n","        self.make_tensors()\n","        # i, j = torch.meshgrid(torch.arange(self.H).to(device), torch.arange(self.W).to(device), indexing='ij')\n","        i, j = torch.meshgrid(torch.arange(self.H), torch.arange(self.W), indexing='ij')\n","        i, j = i.transpose(1, 0), j.transpose(1, 0)\n","        dirs = torch.stack([(i-self.W*0.5)/self.f, -(j-self.H*0.5)/self.f, -torch.ones_like(i)], -1)\n","\n","        rays_d = torch.sum(dirs[..., None, :] * self.pose[:3, :3], -1)\n","        rays_o = torch.broadcast_to(self.pose[:3, -1], rays_d.shape)\n","        self.rays_o = rays_o\n","        self.rays_d = rays_d\n","\n","        return rays_o, rays_d\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"QW3F9xCiWJXR","executionInfo":{"status":"ok","timestamp":1701034169690,"user_tz":300,"elapsed":119,"user":{"displayName":"Shlok Agarwal","userId":"03636398256053486916"}}},"outputs":[],"source":["# takes a list of frames and samples all the rays for the frames uniformly and returns the rays and the depth values\n","# could make it a method of the FrameManager class\n","def sample_frame(frames, num_samples, near, far, dev = 'cuda:0'):\n","\n","    sample_space = torch.linspace(0., 1., num_samples, device=dev)\n","    depth = near*(1.-sample_space) + far*sample_space\n","    mid_depth = (depth[1:] + depth[:-1])/2\n","    rand_sampling = torch.rand([num_samples], device=dev)\n","    upper_sample = torch.cat([mid_depth, depth[-1:]], dim=-1)\n","    lower_sample = torch.cat([depth[:1], mid_depth], dim=-1)\n","    depth_value = lower_sample + rand_sampling * (upper_sample - lower_sample)\n","    depth_value = depth_value.expand(list(frames.rays_o.shape[:-1]) +[num_samples])\n","    #pts are the points on the ray in the format (width, height, n_samples, 3)\n","    pts = frames.rays_o[..., None, :] + frames.rays_d[..., None, :] * depth_value[..., None]\n","    frames.samples = pts\n","    frames.depth_values = depth_value\n","    return pts, depth_value\n","\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"eKg1mJJAWJXR","executionInfo":{"status":"ok","timestamp":1701034170886,"user_tz":300,"elapsed":141,"user":{"displayName":"Shlok Agarwal","userId":"03636398256053486916"}}},"outputs":[],"source":["class Model(nn.Module):\n","\n","    def __init__(self):\n","\n","        super().__init__()\n","\n","        self.input_layer = nn.Linear(60, 256)\n","        self.hidden_layer_block_1 = nn.ModuleList([nn.Linear(256, 256) for i in range(4)])\n","        self.skip_connection_layer = nn.Linear(316,256)\n","        self.hidden_layer_block_2 = nn.ModuleList([nn.Linear(256, 256) for i in range(2)])\n","        self.density_output_layer = nn.Linear(256, 257)\n","        self.last_layer = nn.Linear(280, 128)\n","        self.rgb_output = nn.Linear(128, 3)\n","\n","\n","    def forward(self, position, direction):\n","\n","        # print(position.size())\n","        # print(direction.size())\n","\n","        encoded_position, encoded_direction = self.positional_encoding(position, direction) # should return shape of (# of samples, 60) and (# of samples, 24)\n","\n","        # print(encoded_position.size())\n","        # print(encoded_direction.size())\n","\n","        input_feature_origin = encoded_position.clone()\n","\n","        x = torch.relu(self.input_layer(encoded_position))\n","        for layer in self.hidden_layer_block_1:\n","            x = torch.relu(layer(x))\n","\n","        skip_connection = torch.cat((input_feature_origin, x), dim=1)\n","        x = torch.relu(self.skip_connection_layer(skip_connection))\n","\n","        for layer in self.hidden_layer_block_2:\n","            x = torch.relu(layer(x))\n","\n","        x = torch.relu(self.density_output_layer(x))\n","\n","        # print(x.size())\n","        density = x[:,-1]\n","        direction_connection = torch.cat((encoded_direction, x[:,:-1]), dim=1)\n","        # print(direction_connection.size())\n","        x = torch.relu(self.last_layer(direction_connection))\n","        color = torch.sigmoid(self.rgb_output(x))\n","\n","        return color, density\n","\n","    def positional_encoding(self, position, direction, L_P = 10, L_D =4):\n","\n","        encoded_position = []\n","        encoded_direction = []\n","\n","        for i in range (L_P):\n","            encoded_position.append(torch.sin(2**i * np.pi * position))\n","            encoded_position.append(torch.cos(2**i * np.pi * position))\n","\n","        for i in range (L_D):\n","            encoded_direction.append(torch.sin(2**i * np.pi * direction))\n","            encoded_direction.append(torch.cos(2**i * np.pi * direction))\n","\n","        # print(torch.cat(encoded_position, dim=1).size())\n","        # print(encoded_direction)\n","\n","\n","        return torch.cat(encoded_position, dim=1), torch.cat(encoded_direction, dim=1)\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"9V3XBz2kWJXR","executionInfo":{"status":"ok","timestamp":1701034171927,"user_tz":300,"elapsed":142,"user":{"displayName":"Shlok Agarwal","userId":"03636398256053486916"}}},"outputs":[],"source":["\n","\n","# assuming that points is a list of size (num_of_rays, num_of_samples, 6) so 3 points for position, 3 for direction for every sample on a ray\n","# render_rays will query the network at each sample point to determine the color and density and that point. We then use volume rendering to determine the final color.\n","# dt is distance between adjacent samples; prob just near-far / num of samples for uniform sampling\n","def render_rays(model, points, dt):\n","\n","    # dt = torch.ones(points.size()[0])/points.size()[0] # just for testing\n","    positions = points[:,:,0:3]\n","    directions = points[:,:,3:]\n","    positions_reshaped = torch.reshape(positions,(-1,3)).to(device)\n","    directions_reshaped = torch.reshape(directions, (-1,3)).to(device)\n","    color, density = model(positions_reshaped, directions_reshaped) # color should be (# of samples * # of rays, 3) density should be (# of samples * # of rays, 1)\n","\n","    color = torch.reshape(color, positions.shape).to(device)\n","    density = torch.reshape(density, (positions.shape[0], positions.shape[1], 1)).to(device)\n","\n","    # print(color.shape)\n","    # print(density.shape)\n","\n","    alpha = 1 - torch.exp(-density*dt)\n","    transmittance = torch.cumprod(torch.exp(-density * dt), dim=1)\n","    # print(transmittance.shape)\n","    # print(alpha.shape)\n","    # print(color.shape)\n","    rgb = torch.sum(alpha * transmittance * color, dim=1)\n","    return rgb\n","\n","\n","def train(model, epochs, data, learning_rate, dt):\n","\n","    optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n","\n","    for i in range(epochs):\n","        print(\"Epoch number\", i)\n","        for batch in data: # rays should be size (# of rays, # of samples along ray, 6) 6 = 3 for position + 3 for direction, actuals_vals should be size (# of rays, # of samples, 3) 3 = rgb values\n","            rays, actual_vals = batch\n","            rays = rays.to(device)\n","            actual_vals = actual_vals.to(device)\n","            predicted = render_rays(model, rays, dt).to(device)\n","            # print(predicted.shape)\n","            # print(actual_vals.shape)\n","            loss = torch.sum((predicted-actual_vals)**2)\n","            # print(loss.shape)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","\n","\n","\n","# model = Model()\n","\n","# one = torch.zeros(10, 3)\n","# two = torch.zeros(10, 3)\n","# # output = model(one, two)\n","# # print(output)\n","# points = torch.rand(50,25,6) # 50 rays, 25 samples per ray, 6\n","# output = render_rays(model, points, 0.1)\n","\n","# data = torch.cat((rays, actual), dim=3)\n","# train(model, 10, (rays,actual), 0.05, .01)"]},{"cell_type":"code","source":["frameManager = FrameManager()\n","!ls drive/MyDrive/EECS\\ 504\\ Final\\ Project/NOVEL_NERF/Data/\n","\n","frameManager.read_frames('drive/MyDrive/EECS 504 Final Project/NOVEL_NERF/Data/lego')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IKcSLoH7Xqm5","executionInfo":{"status":"ok","timestamp":1701034182630,"user_tz":300,"elapsed":9846,"user":{"displayName":"Shlok Agarwal","userId":"03636398256053486916"}},"outputId":"dc12e8e2-fa8b-4edf-ae39-02756e9f26a7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["lego\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-3-7c13af1346ab>:59: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n","  img = imageio.imread(img_file)\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.imshow(frameManager.test_frames[0].img)\n","test_rays, test_directions = frameManager.prepare_test_data()\n","\n"],"metadata":{"id":"kTwmXS0vbe24"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mwVgu_aSWJXS","executionInfo":{"status":"aborted","timestamp":1701032166701,"user_tz":300,"elapsed":1,"user":{"displayName":"Shlok Agarwal","userId":"03636398256053486916"}}},"outputs":[],"source":["### only to test the model with random input\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","class NeRFDataset(Dataset):\n","    def __init__(self, rays, target_images):\n","        self.rays = rays\n","        self.target_images = target_images\n","\n","    def __len__(self):\n","        return len(self.rays)\n","\n","    def __getitem__(self, index):\n","        return self.rays[index], self.target_images[index]\n","\n","rays = torch.randn(50,25,6)\n","actual = torch.randn(50,3)\n","\n","nerf_dataset = NeRFDataset(rays, actual)\n","batch_size = 32\n","train_data_loader = DataLoader(nerf_dataset, batch_size=batch_size, shuffle=True)\n","train(model, 10, train_data_loader, 0.05, 0.01)"]},{"cell_type":"code","source":[],"metadata":{"id":"R6_al2G6av7-"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.9"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}