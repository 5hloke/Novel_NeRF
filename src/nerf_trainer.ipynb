{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74b4203-648c-4b53-ac52-fcdb6f03c7ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install matplotlib\n",
    "!pip install imageio\n",
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "import torch\n",
    "import os\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import imageio\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "# Make sure to switch runtime to the GPU\n",
    "import gc\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "print(device)\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d829e0a-1c36-45de-8653-d27cbfe09c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'Data/lego'\n",
    "DATA = np.load('tiny_nerf_data.npz')\n",
    "class FrameManager:\n",
    "    def __init__(self):\n",
    "        self.test_frames = []\n",
    "        self.train_frames = []\n",
    "        self.val_frames = []\n",
    "        self.cam_angle = 0\n",
    "        self.f = None\n",
    "        self.H = None\n",
    "        self.W = None\n",
    "\n",
    "\n",
    "    def read_frames(self, path):\n",
    "        img = path['images']\n",
    "        poses = path['poses']\n",
    "        self.f = path['focal']\n",
    "                # img = resize(img, (100, 100)) ### image scaled down to test\n",
    "        self.H, self.W = img.shape[0], img.shape[1]\n",
    "    \n",
    "        # new_frame = Frame(img, np.array(frame['transform_matrix']), self.f)\n",
    "        for i in range(len(img)):\n",
    "            new_frame = Frame(img[i], poses[i], self.f)\n",
    "            self.train_frames.append(new_frame)\n",
    "\n",
    "class Frame:\n",
    "    def __init__(self, image, pose, f):\n",
    "        self.img = image\n",
    "        self.pose = pose\n",
    "        self.H, self.W = image.shape[0], image.shape[1]\n",
    "        self.f = torch.from_numpy(f).to('cuda')\n",
    "        self.samples = None\n",
    "        self.rays_o = None\n",
    "        self.rays_d = None\n",
    "        self.depth_values = None\n",
    "\n",
    "    def copy_to_device(self, device):\n",
    "        self.img = torch.from_numpy(self.img).to(device)\n",
    "        self.pose = torch.from_numpy(self.pose).to(device)\n",
    "\n",
    "    def make_tensors(self):\n",
    "        if not(torch.is_tensor(self.img)):\n",
    "            self.img = torch.from_numpy(self.img)\n",
    "        if not(torch.is_tensor(self.pose)):\n",
    "            self.pose = torch.from_numpy(self.pose)\n",
    "        self.img = self.img.to(torch.float64)\n",
    "        self.pose = self.pose.to(torch.float64)\n",
    "    \n",
    "\n",
    "    # function to get the rays from the image through every pixel of the Camera (Using Pytorch) on GPU\n",
    "    # Assuming a pinhole camera model\n",
    "    def get_rays(self, device):\n",
    "        # self.copy_to_device(device)\n",
    "        self.make_tensors()\n",
    "        i, j = torch.meshgrid(torch.arange(self.H).to(device), torch.arange(self.W).to(device), indexing='ij')\n",
    "        # i, j = torch.meshgrid(torch.arange(self.H), torch.arange(self.W), indexing = 'ij')\n",
    "        i, j = i.transpose(-1, -2), j.transpose(-1, -2)\n",
    "        dirs = torch.stack([(i-self.W*0.5)/self.f, -(j-self.H*0.5)/self.f, -torch.ones_like(i)], -1)\n",
    "        if (self.pose.device != device):\n",
    "            self.pose = self.pose.to(device)\n",
    "        # print(dirs.device)\n",
    "        rays_d = torch.sum(dirs[..., None, :] * self.pose[:3, :3], -1)\n",
    "        rays_o = torch.broadcast_to(self.pose[:3, -1], rays_d.shape)\n",
    "        # self.pose = self.pose.to(\"cpu\")\n",
    "        self.rays_o = rays_o\n",
    "        self.rays_d = rays_d\n",
    "        \n",
    "        # del i\n",
    "        # del j\n",
    "        return rays_o.view([-1, 3]), rays_d.view([-1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2730192-64dd-4d9b-81cd-77e0917fa8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_frame(frames, num_samples, near, far, dev = 'cuda'):\n",
    "\n",
    "    sample_space = torch.linspace(0., 1., num_samples, device=dev)\n",
    "    rays_o = frames.rays_o.reshape([-1, 3])\n",
    "    rays_d = frames.rays_d.reshape([-1, 3])\n",
    "    # sample_space = torch.linspace(0., 1., num_samples)\n",
    "    depth = near*(1.-sample_space) + far*sample_space\n",
    "    mid_depth = (depth[1:] + depth[:-1])/2\n",
    "    rand_sampling = torch.rand([num_samples], device=dev)\n",
    "    # rand_sampling = torch.rand([num_samples])\n",
    "    upper_sample = torch.cat([mid_depth, depth[-1:]], dim=-1)\n",
    "    lower_sample = torch.cat([depth[:1], mid_depth], dim=-1)\n",
    "    depth_value = lower_sample + rand_sampling * (upper_sample - lower_sample)\n",
    "    depth_value = depth_value.expand(list(rays_o.shape[:-1]) +[num_samples])\n",
    "    #pts are the points on the ray in the format (width, height, n_samples, 3)\n",
    "    pts = rays_o[..., None, :] + rays_d[..., None, :] * depth_value[..., :, None]\n",
    "    frames.samples = pts\n",
    "    frames.depth_values = depth_value\n",
    "    # del rand_sampling\n",
    "    # del sample_space\n",
    "    return pts, depth_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114ebd38-e126-4ad2-9b68-db468c584331",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(60, 256, dtype=torch.float64)\n",
    "        self.hidden_layer_block_1 = nn.ModuleList([nn.Linear(256, 256, dtype=torch.float64) for i in range(4)])\n",
    "        self.skip_connection_layer = nn.Linear(316,256, dtype=torch.float64)\n",
    "        self.hidden_layer_block_2 = nn.ModuleList([nn.Linear(256, 256, dtype=torch.float64) for i in range(2)])\n",
    "        self.density_output_layer = nn.Linear(256, 256, dtype=torch.float64)\n",
    "        self.rgb_layer = nn.Linear(256, 256, dtype=torch.float64)\n",
    "        self.last_layer = nn.Linear(280, 128, dtype=torch.float64)\n",
    "        self.rgb_output = nn.Linear(128, 3, dtype=torch.float64)\n",
    "\n",
    "\n",
    "    def forward(self, position, direction):\n",
    "\n",
    "        # print(position.size())\n",
    "        # print(direction.size())\n",
    "\n",
    "        encoded_position, encoded_direction = self.positional_encoding(position, direction) # should return shape of (# of samples, 60) and (# of samples, 24)\n",
    "        # print(encoded_position.dtype)\n",
    "        # print(encoded_direction.dtype)\n",
    "        # print(encoded_position.size())\n",
    "        # print(encoded_direction.size())\n",
    "        # input_feature_origin = encoded_position.clone().to(device)\n",
    "        x = nn.functional.relu(self.input_layer(encoded_position))\n",
    "        # print(x.dtype)\n",
    "        for layer in self.hidden_layer_block_1:\n",
    "            x = nn.functional.relu(layer(x))\n",
    "        # print(x.dtype)\n",
    "        skip_connection = torch.cat((encoded_position, x), dim=1)\n",
    "        x = nn.functional.relu(self.skip_connection_layer(skip_connection))\n",
    "        # print(x.dtype)\n",
    "        for layer in self.hidden_layer_block_2:\n",
    "            x = nn.functional.relu(layer(x))\n",
    "        # print(x.dtype)\n",
    "        dens_x = nn.functional.relu(self.density_output_layer(x))\n",
    "        # print(x.dtype)\n",
    "        # print(x.size())\n",
    "        density = dens_x[:,-1]\n",
    "        x = self.rgb_layer(x)\n",
    "        x = torch.cat([x, encoded_direction], dim=-1)\n",
    "        # print(direction_connection.size())\n",
    "        x = nn.functional.relu(self.last_layer(x))\n",
    "        # print(x.dtype)\n",
    "        color = torch.sigmoid(self.rgb_output(x))\n",
    "        \n",
    "        del encoded_position\n",
    "        del encoded_direction\n",
    "        # del direction_connection\n",
    "        \n",
    "        return color, density\n",
    "\n",
    "    def positional_encoding(self, position, direction, L_P = 10, L_D =4):\n",
    "        # direction = direction / torch.norm(direction, dim=-1, keepdim=True)\n",
    "        direction = direction[:, None, ...].expand(position.shape).reshape((-1, 3))\n",
    "        position = position.reshape([-1, 3])\n",
    "        \n",
    "        encoded_position = []\n",
    "        encoded_direction = []\n",
    "\n",
    "        for i in range (L_P):\n",
    "            encoded_position.append(torch.sin(2**i * np.pi * position))\n",
    "            encoded_position.append(torch.cos(2**i * np.pi * position))\n",
    "\n",
    "        for i in range (L_D):\n",
    "            encoded_direction.append(torch.sin(2**i * np.pi * direction))\n",
    "            encoded_direction.append(torch.cos(2**i * np.pi * direction))\n",
    "\n",
    "        # print(torch.cat(encoded_position, dim=1).size())\n",
    "        # print(encoded_direction)\n",
    "\n",
    "        return torch.cat(encoded_position, dim=1), torch.cat(encoded_direction, dim=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99405f51-54c3-4518-b783-b356494c8d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def render_rays(model, frame):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    # dt = torch.ones(points.size()[0])/points.size()[0] # just for testing\n",
    "    positions = frame.samples\n",
    "    directions = frame.rays_d.reshape([-1, 3])\n",
    "    color, density = model(positions, directions) # color should be (# of samples * # of rays, 3) density should be (# of samples * # of rays, 1)\n",
    "    # rendering begins here\n",
    "    dt = frame.depth_values[..., 1:] - frame.depth_values[..., :-1]\n",
    "    dt = torch.cat([dt, 1e20*torch.ones_like(dt[..., :1])], dim = -1).to(device) #torch.Size([800, 800, 8])\n",
    "    \n",
    "    color = torch.reshape(color, frame.samples.shape)#.to(device)\n",
    "    density = torch.reshape(density, (frame.samples.shape[0], frame.samples.shape[1]))#.to(device)\n",
    "    alpha = 1 - torch.exp(-density*dt)\n",
    "    transmittance = torch.cumprod((1-alpha), dim=1)\n",
    "    transmittance = torch.roll(transmittance, 1, -1)\n",
    "    transmittance[..., 0] = 1\n",
    "\n",
    "    # print(transmittance.shape)\n",
    "    # print(alpha.shape)\n",
    "    # print(color.shape)\n",
    "    rgb = torch.sum(alpha[:,:,None] * transmittance[:,:,None] * color, dim=-2)\n",
    "    del dt\n",
    "    return rgb\n",
    "\n",
    "\n",
    "def train(model, epochs, data, learning_rate = 5e-4):\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    # print()\n",
    "    index_test = 9\n",
    "    count = 0\n",
    "    for fr in data.train_frames:\n",
    "        # print(\"Getting rays and sampling\")\n",
    "        fr.get_rays(device)\n",
    "        # sample_frame(fr, 100, 2, 6, dev = 'cuda:0')\n",
    "        # print(count)\n",
    "        count +=1\n",
    "    plt.imshow(data.train_frames[index_test].img)\n",
    "    plt.savefig(f'test_img_{index_test}_original.png')\n",
    "    plt.close()\n",
    "\n",
    "    ls = []\n",
    "    for i in range(epochs):\n",
    "      model.train()\n",
    "      print(\"Epoch number\", i)\n",
    "      index = np.random.randint(len(data.train_frames))\n",
    "      fr = data.train_frames[index]\n",
    "      sample_frame(fr, 64, 2, 6, dev = 'cuda:0')\n",
    "      # print(fr.img.shape)\n",
    "      # fr.get_rays(device)\n",
    "      # sample_frame(fr, 8, 1, 10, dev = 'cuda:0)\n",
    "      predicted = render_rays(model, fr)\n",
    "      # print(predicted.device)\n",
    "      actual = fr.img.to(device)\n",
    "      # print(fr.img.device)\n",
    "      loss = torch.nn.functional.mse_loss(predicted.reshape([actual.shape[0], actual.shape[1], actual.shape[2]]), actual[:, :, :]) # fr.img is (800,800,4)\n",
    "      print(\"Loss:\", loss.item())\n",
    "      ls.append(loss.item())\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      if i % 25 == 0:\n",
    "        torch.save(model.state_dict(), 'nerf_3.pt')\n",
    "        test(model, data, index_test,i, loss) ## test on same test image each epoch\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def test(model, data, index_test, epoch, loss):\n",
    "    model.eval()\n",
    "    fr = data.train_frames[index_test]\n",
    "    # img = fr.img.to(device)\n",
    "    # fr.get_rays(device)\n",
    "    sample_frame(fr, 64, 2, 6, dev = 'cuda:0')\n",
    "    predicted = render_rays(model, fr)\n",
    "    predicted = predicted.detach().cpu().numpy()\n",
    "    # loss = torch.nn.functional.mse_loss(predicted.reshape([fr.img.shape[0], fr.img.shape[1], fr.img.shape[2]-1]), img[:, :, :-1])\n",
    "    # print(\"Loss:\", loss.item())\n",
    "    # if(epoch%25 == 0):\n",
    "        # plt.imshow(fr.img)\n",
    "    plt.imshow(predicted.reshape([100, 100, 3]))\n",
    "    plt.savefig(f'test_img_{index_test}_epoch_{epoch}.png')\n",
    "    plt.close()\n",
    "    del predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51316974-179f-45ba-a5ac-0fc7fef460d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "frameManager = FrameManager()\n",
    "frameManager.read_frames(DATA)\n",
    "# frameManager.read_frames('drive/MyDrive/EECS 504 Final Project/NOVEL_NERF/Data/lego')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d086c7-027e-466b-b32e-ae886f176370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load('nerf_3.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546645f9-b858-4557-b112-636d40f36a12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = train(model, 10000, frameManager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481faa18-2812-42a8-9008-531b948c37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'nerf.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145fcf2-0e93-48a6-884e-4a142c8d1ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
